TO SIEVE ON A CLUSTER WITH OAR SCHEDULER IN MODE BESTEFFORT

$cdir : working directory on the cluster (contains $name.poly)
$ldir : working directory on the local machine (contains $name.poly)

I) on the cluster :
- initialization of the working directory :
$ ./init_cdir $cdir $bindir $nb_jobs_by_node $opt
  example with a cluster of 8 cores per node :
  $ ./init_cdir $HOME/snfs250 $HOME/cado-nfs/build/$HOSTNAME 4 "-mt 2 -I 15 -ratq"

- run cut_n_roots to create waiting list of jobs:
$ cd $cdir/inqueue
$ ../bin/cut_n_roots -poly $cdir/$name.poly -q0 $q0 -q1 $q1 -N $N | xargs touch
  (split [q0, q1[ into maximal intervals of N special q's each)
NB : fill the inqueue directory step-by-step to avoid that it contains
     a lot of files.

- run several jobs oarsub (daemon):
$ cd $cdir/log_oar
$ ../scripts/run_oarjobs $sizepack $nbpack &
  ( number of nodes = $sizepack * $nbpack )
  example with a cluster of 133 nodes :
  $ ../scripts/run_oarjobs 10 13 &


II) on the local machine :
- initialization of the working directory :
$ ./init_ldir $ldir $bindir

- synchronize the output_rels directories:
$ rsync -av $name_cluster:$cdir/output_rels/ $ldir/output_rels/

- merges the relations files:
$ $ldir/scripts/merge.pl $ldir/output_rels/ -o $ldir/merge_rels/ -n $name --range $q0-$q1
  (merge the relations files between $q0 and $q1 by interval of 1 million)
NB : you can clean the files in the output_rels directories step-by-step 
     to avoid that they contain a lot of files.

- verify that you have enough of relations :
$ $ldir/scripts/check_dup_purge.pl
  (execute check_rels, dup1, dup2 and purge)
NB : - don't kill the script during check_rels and dup1.
     - if the file $name.purged exists then you have enough of relations 

- if you have enough of relations : 
$ touch $ldir/$name.polysel_done $ldir/$name.factbase_done $ldir/$name.freerels_done $ldir/$name.sieve_done $ldir/$name.dup_done $ldir/$name.purge_done
then run cadofactor.pl.

- run cpu_time.pl to estimate the total cpu time for sieving.
$ $ldir/cpu_time.pl
