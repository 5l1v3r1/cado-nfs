cadofactor and OAR HowTo:

1. Running cadofactor inside the OAR job:

This is arguably the easiest to set up. It allows cadofactor to get the list
of hostnames on which to launch slaves directly from OAR by using the
parameter

slaves.hostnames = @${OAR_NODE_FILE}

which reads the list of host names from the file specified in the shell
environment variable OAR_NODE_FILE. That file lists each host 32 times, as
each node has 32 virtual CPUs. It's more sensible to use only 8 or 16
clients per host, with

slaves.nrclients = 16

Slaves on catrel nodes need to be contacted via oarsh instead of SSH, which
is done with

slaves.ssh.execbin = oarsh

A working example of parameters for OAR is in parameters.oar.

As oarsub expects the command to run as a single parameter, a wrapper script
needs to be used for starting cadofactor.py with the required parameters;
alternatively an interactive session can be used in which to run cadofactor.

Of course, additional clients can be started on other CATREL nodes manually
to help with the computation.



2. Running cadofactor outside the OAR job

If cadofactor runs on a machine outside Grid5000/fcatrel, only the slaves
need to be started via OAR. They need the URL of the server, and the
server's certificate fingerprint (once HTTPS is implemented). A simple
example shell script to launch slaves is in start_clients.sh.



3. Full OAR integration with automatic job submission

This is TBD. The plan is:

A submission script is running on the OAR front end node and queries the
cadofactor script, running outside the cluster, for which task is currently
being processed. For each task, it has job submission parameters, giving the
number of nodes, number of clients per node, submission mode (besteffort or
not, etc.), etc. It submits jobs according to these parameters, monitors the
jobs, re-submits cancelled ones, or cancels them when the task for which
they were started has finished.

The individual submissions work pretty much like case 2. above; the
specified number of client scripts is run on each node; the client scripts
process work units.

To integrate the linear algebra task with this automatic submission, a
single workunit for the bwc.pl run will be generated, listing all the input
files for bwc.pl as URLs. bwc.pl has the capability of downloading its input
files via HTTP, so it can download them from the workunit server. It also
has the capability of automatically restarting an interrupted run, and of
querying OAR for submission parameters to parallelize/distribute its
operation automatically.

Thus, when the linear algebra step has started, the front end script will
submit a job and launch a single workunit client inside. That client
downloads the single Linear Algebra workunit, which starts bwc.pl. Then
bwc.pl downloads its inputs via HTTP from the workunit client, and runs the
linear algebra programs. The result files are uploaded to the workunit
server like for other workunits.
