#ifndef BUCKET_H_
#define BUCKET_H_

/*
 * Bucket sieving: radix-sort sieve updates as they are created.
 */

#include <stdint.h>
#include "cado-endian.h"
// #define SAFE_BUCKETS
#ifdef SAFE_BUCKETS
#include <stdio.h>
#include <limits>
#include "portability.h"
#endif
#include "misc.h"
#include "fb-types.h"
#include "fb.h"

/*
 * This bucket module provides a way to store elements (that are called
 * updates), while partially sorting them, according to some criterion (to
 * be defined externally): the incoming data is stored into several
 * buckets. The user says for each data to which bucket it belongs. This
 * module is supposed to perform this storage in a cache-friendly way and
 * so on...
 */

/*
 * Main available commands are 
 *   push_bucket_update(i, x)  :   add x to bucket number i
 *   get_next_bucket_update(i) :   iterator to read contents of bucket number i
 *
 * See the MAIN FUNCTIONS section below for the complete interface, with
 * prototypes of exported functions.
 */

/********  Data structure for the contents of buckets **************/

/* In principle, the typedef for the bucket_update can be changed without
 * affecting the rest of the code. 
 */

/*
 * For the moment, we store the bucket updates and a 16-bit field
 * that can contain, for instance, the low bits of p.
 */

template <typename TARGET_TYPE, typename SOURCE_TYPE>
TARGET_TYPE limit_cast(const SOURCE_TYPE &b)
{
  ASSERT_EXPENSIVE(b >= std::numeric_limits<TARGET_TYPE>::min());
  ASSERT_EXPENSIVE(b <= std::numeric_limits<TARGET_TYPE>::max());
  return static_cast<TARGET_TYPE>(b);
}

/* THE ORDER IS MANDATORY! */
class bucket_update_shorthint_t {
public:
  static const size_t bucket_region = BUCKET_REGION;
#ifdef CADO_LITTLE_ENDIAN
  slice_offset_t hint;
  uint16_t x;
  bucket_update_shorthint_t(){}
  bucket_update_shorthint_t(const uint64_t offset, const slice_offset_t slice_offset)
    : hint(slice_offset), x(limit_cast<uint16_t>(offset)) {}
  bucket_update_shorthint_t(const uint64_t offset, const fbprime_t p MAYBE_UNUSED, const slice_offset_t slice_offset, const slice_index_t slice_index MAYBE_UNUSED)
    : hint(slice_offset), x(limit_cast<uint16_t>(offset)) {}
#else
  uint16_t x;
  slice_offset_t hint;
  bucket_update_shorthint_t(){}
  bucket_update_shorthint_t(const uint64_t offset, const slice_offset_t slice_offset)
    : x(limit_cast<uint16_t>(offset)), hint(slice_offset) {}
  /* Uniform interface for all update types, for use in templated functions */
  bucket_update_shorthint_t(const uint64_t offset, const fbprime_t p MAYBE_UNUSED, const slice_offset_t slice_offset, const slice_index_t slice_index MAYBE_UNUSED)
    : x(limit_cast<uint16_t>(offset)), hint(slice_offset) {}
#endif
};

/* An update with the complete prime, generated by line re-sieving */

class bucket_update_prime_t {
public:
  static const size_t bucket_region = BUCKET_REGION;
  uint16_t x;
  fbprime_t p;
  bucket_update_prime_t(){}
  bucket_update_prime_t(const uint64_t offset, const fbprime_t p)
    : x(limit_cast<uint16_t>(offset)), p(p) {}
  bucket_update_prime_t(const uint64_t offset, const fbprime_t p, const slice_offset_t slice_offset MAYBE_UNUSED, const slice_index_t slice_index MAYBE_UNUSED)
    : x(limit_cast<uint16_t>(offset)), p(p) {}
};

/* When purging a bucket, we don't store pointer arrays to indicate where in
   the puged data a new slice begins, as each slice will have only very few
   updates surviving. Instead, we re-write each update to store both slice
   index and offset. */

class bucket_update_longhint_t {
public:
  static const size_t bucket_region = BUCKET_REGION;
  uint16_t x;
  slice_index_t index;
  slice_offset_t hint;
  bucket_update_longhint_t(){}
  bucket_update_longhint_t(const uint64_t offset, const slice_offset_t slice_offset, const slice_index_t slice_index)
    : x(limit_cast<uint16_t>(offset)), index(slice_index), hint(slice_offset) {}
  bucket_update_longhint_t(const uint64_t offset, const fbprime_t p MAYBE_UNUSED, const slice_offset_t slice_offset, const slice_index_t slice_index)
    : x(limit_cast<uint16_t>(offset)), index(slice_index), hint(slice_offset) {}
};


/******** Bucket array typedef **************/
/******** Bucket array implementation **************/
template <typename UPDATE_TYPE>
class bucket_array_t : private NonCopyable {
  static const size_t bucket_region = UPDATE_TYPE::bucket_region;
  UPDATE_TYPE *big_data;
  size_t big_size;                    // size of bucket update memory

  UPDATE_TYPE ** bucket_write;    // Contains pointers to first empty
                                      // location in each bucket
  UPDATE_TYPE ** bucket_start;    // Contains pointers to beginning of
                                      // buckets
  UPDATE_TYPE ** bucket_read;     // Contains pointers to first unread
                                      // location in each bucket
  slice_index_t    * slice_index;     // For each slice that gets sieved,
                                      // new index is added here
  UPDATE_TYPE ** slice_start;     // For each slice there are
                                      // n_bucket pointers, each pointer
                                      // tells where in the corresponding 
                                      // bucket the updates from that slice
                                      // start
  uint32_t           n_bucket;        // Number of buckets
  uint64_t           bucket_size;     // The allocated size of one bucket.
  size_t             size_b_align;    // (sizeof(void *) * n_bucket + 63) & ~63
                                      // to align bucket_* on cache line
  slice_index_t      nr_slices;       // Number of different slices
  slice_index_t      alloc_slices;    // number of checkpoints (each of size
                                      // size_b_align) we have allocated

  static const slice_index_t initial_slice_alloc = 256;
  static const slice_index_t increase_slice_alloc = 128;

  /* Get a pointer to the pointer-set for the i_slice-th slice */
  UPDATE_TYPE ** get_slice_pointers(const slice_index_t i_slice) const {
    ASSERT_ALWAYS(i_slice < nr_slices);
    ASSERT_ALWAYS(size_b_align % sizeof(UPDATE_TYPE *) == 0);
    return (slice_start + i_slice * size_b_align / sizeof(UPDATE_TYPE *));
  }
  size_t nb_of_updates(const int i) const {
      return (bucket_write[i] - bucket_start[i]);
  }
  void realloc_slice_start(size_t);
public:
  /* Constructor sets everything to zero, and does not allocate memory.
     allocate_memory() does all the allocation. */
  bucket_array_t();
  /* Destructor frees memory, if memory was allocated. If it wasn't, it's
     basically a no-op, except for destroying the bucket_array_t itself. */
  ~bucket_array_t();

  /* Lacking a move constructor before C++11, we make a fake one. We use this
     to store the bucket_array_t on the stack in fill_in_buckets(), to remove
     one level of pointer dereferencing.
     This method copies all the fields of other, then sets them to 0 in other.
     Deconstructing the other bucket_array_t after move() is a no-op, as if
     other had been constructed, but never run allocate_memory(). */
  void move(bucket_array_t &other);

  /* Allocate enough memory to be able to store at least _n_bucket buckets,
     each of at least _bucket_size entries. If at least as much memory had
     already been allocated, does not resize it.  */
  void allocate_memory(const uint32_t _n_bucket, const size_t _bucket_size,
                       const slice_index_t prealloc_slices = initial_slice_alloc);
  /* Return a begin iterator over the UPDATE_TYPE entries in i_bucket-th
     bucket, generated by the i_slice-th slice */
  const UPDATE_TYPE *begin(const size_t i_bucket, const slice_index_t i_slice) const {
    ASSERT_ALWAYS(i_slice < nr_slices);
    const UPDATE_TYPE * const p = get_slice_pointers(i_slice)[i_bucket];
    /* The first slice we wrote must start at the bucket start */
    ASSERT_ALWAYS(i_slice != 0 || p == bucket_start[i_bucket]);
    return p;
  }
  /* Return an end iterator over the UPDATE_TYPE entries in i_bucket-th
     bucket, generated by the i_slice-th slice */
  const UPDATE_TYPE *end(const size_t i_bucket, const slice_index_t i_slice) const {
    ASSERT_ALWAYS(i_slice < nr_slices);
    return (i_slice + 1 < nr_slices) ? get_slice_pointers(i_slice + 1)[i_bucket] :
      bucket_write[i_bucket];
  }

  void reset_pointers();
  slice_index_t get_nr_slices() const {return nr_slices;}
  slice_index_t get_slice_index(const slice_index_t i_slice) const {
    ASSERT_ALWAYS(i_slice < nr_slices);
    return slice_index[i_slice];
  }
  void add_slice_index(const slice_index_t new_slice_index) {
    /* Write new set of pointers for the new factor base slice */
    ASSERT_ALWAYS(nr_slices <= alloc_slices);
    if (nr_slices == alloc_slices) {
      /* We're out of allocated space for the checkpoints. Realloc to bigger
         size. We add space for increase_slice_alloc additional entries. */
      realloc_slice_start(increase_slice_alloc);
    }
    aligned_medium_memcpy((uint8_t *)slice_start + size_b_align * nr_slices, bucket_write, size_b_align);
    slice_index[nr_slices++] = new_slice_index;
  }
  double max_full () const;
  /* Push an update to the designated bucket. Also check for overflow, if
     SAFE_BUCKETS is defined. */
  void push_update(const int i, const UPDATE_TYPE &update) {
#ifdef SAFE_BUCKETS
      if (bucket_start[i] + bucket_size == bucket_write[i]) {
          fprintf(stderr, "# Warning: hit end of bucket nb %d\n", i);
          return;
      }
#endif
      *bucket_write[i]++ = update;
  }
  /* Create an update for a hit at location offset and push it to the
     coresponding bucket */
  void push_update(const uint64_t offset, const fbprime_t p,
      const slice_offset_t slice_offset, const slice_index_t slice_index)
  {
    const uint64_t bucket_number = offset / bucket_region;
    ASSERT_EXPENSIVE(bucket_number < n_bucket);
    UPDATE_TYPE update(offset % bucket_region, p, slice_offset, slice_index);
#if 0 && defined(TRACE_K)
    /* TODO: don't define where_am_I_ptr in las-types.h, to avoid cyclic header
       file dependencies */
    /* TODO: need to be able to set the current region size in WHERE_AM_I,
       so we can compute N * regionsize + offset correctly for different
       sieving levels */
    WHERE_AM_I_UPDATE(w, N, bucket_number);
    WHERE_AM_I_UPDATE(w, x, update.x);
    
    if (trace_on_spot_x(offset)) {
        verbose_output_print (TRACE_CHANNEL, 0, "# Pushed factor base entry (slice_index=%u, slice_offset=%u, p=%"
                              FBPRIME_FORMAT "), hit at location (x=%u, %s) to BA[%u]\n",
                 (unsigned int) slice_index, (unsigned int) slice_offset, p,
                 (unsigned int) update.x, sidenames[side],
                 (unsigned int) bucket_number);
        ASSERT(test_divisible(w));
      }
#endif
    push_update(bucket_number, update);
  }
};


/* A class that stores updates in a single "bucket".
   It's really just a container class with pre-allocated array for storage,
   a persistent read and write pointer. */
template <class UPDATE_TYPE>
class bucket_single {
  UPDATE_TYPE *start; /* start is a "strong" reference */
  UPDATE_TYPE *read;  /* read and write are "weak" references into the allocated memory */
  UPDATE_TYPE *write;
  size_t _size;
public:
  bucket_single (const size_t size) : _size(size)
  {
    start = new UPDATE_TYPE[size];
    read = start;
    write = start;
  }

  ~bucket_single() {
    delete[] start;
    start = read  = write = NULL;
    _size = 0;
  }

  /* A few of the standard container methods */
  const UPDATE_TYPE * begin() const {return start;}
  const UPDATE_TYPE * end() const {return write;}
  size_t size() const {return write - start;}

  /* Main writing function: appends update to bucket number i.
   * If SAFE_BUCKETS is not #defined, then there is no checking that there is
   * enough room for the update. This could lead to a segfault, with the
   * current implementation!
   */
  void push_update (const UPDATE_TYPE &update)
  {
      *(write++) = update;
#ifdef SAFE_BUCKETS
      if (start + _size <= write) {
          fprintf(stderr, "# Warning: hit end of bucket\n");
          write--;
      }
#endif
  }
  const UPDATE_TYPE &get_next_update () {
#ifdef SAFE_BUCKETS
    ASSERT_ALWAYS (read < write);
#endif
    return *read++; 
  }
  void rewind_by_1() {if (read > start) read--;}
  bool is_end() const { return read == write; }

  void sort ();
};

/* Stores info containing the complete prime instead of only the low 16 bits */

class bucket_primes_t : public bucket_single<bucket_update_prime_t> {
    typedef bucket_single<bucket_update_prime_t> super;
public:  
  bucket_primes_t (const size_t size) : super(size){}
  ~bucket_primes_t(){}
  void purge (const bucket_array_t<bucket_update_shorthint_t> &BA, 
          int i, const fb_part *fb, const unsigned char *S);
};

/* Stores info containing both slice index and offset instead of only the offset */

class bucket_array_complete : public bucket_single<bucket_update_longhint_t> {
    typedef bucket_single<bucket_update_longhint_t> super;
public:  
  bucket_array_complete (const size_t size) : super(size){}
  ~bucket_array_complete(){}
  void purge (const bucket_array_t<bucket_update_shorthint_t> &BA, int i, const unsigned char *S);
};


/* Compute a checksum over the bucket region.

   We import the bucket region into an mpz_t and take it modulo
   checksum_prime. The checksums for different bucket regions are added up,
   modulo checksum_prime. This makes the combined checksum independent of
   the order in which buckets are processed, but it is dependent on size of
   the bucket region. Note that the selection of the sieve region, i.e., of J
   depends somewhat on the number of threads, as we want an equal number of
   bucket regions per thread. Thus the checksums are not necessarily
   comparable between runs with different numbers of threads. */

class sieve_checksum {
  static const unsigned int checksum_prime = 4294967291u; /* < 2^32 */
  unsigned int checksum;
  void update(const unsigned int);

  public:
  sieve_checksum() : checksum(0) {}
  unsigned int get_checksum() {return checksum;}

  /* Combine two checksums */ 
  void update(const sieve_checksum &other) {
    /* Simply (checksum+checksum2) % checksum_prime, but using
       ularith_addmod_ul_ul() to handle sums >= 2^32 correctly. */
    this->update(other.checksum);
  }
  /* Update checksum with the pointed-to data */
  void update(const unsigned char *, size_t);
};

#endif	/* BUCKET_H_ */
