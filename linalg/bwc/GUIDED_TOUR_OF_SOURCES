The pi_go() function
--------------------

This is the main workhorse for the parallelizing of the computation. It
allows multi-thread (with pthread) and multinode (with MPI) processing.
Some of the calls offered with the parallelizing_info structure are
reminiscent of the MPI calling interface.

The parameters given to pi_go() are:
   . an outer description of how many mpi jobs and threads should be set up.
   . a pointer to the function that must be called in parallel.
   . arguments which are to be passed to the function.

Its implementation is in parallelizing_info.[ch] .  The main data
structure that conveys information on the parallel setting is 
  struct parallelizing_info_s
The details are not so simple... but in a nutshell, we get 3
communicators, the meaning of which is more or less a generalization of
"MPI_Comm" to encompass also the pthread level. The first communicator
allows full communication, the second is for row-wise communications, the
third is for column-wise communications.

The prototype of the function passed to pi_go() is
  void *(*fcn)(parallelizing_info_ptr pi, void * arg)
where parallelizing_info_ptr pi is built by pi_go and passed to the
function. The last argument allows any additional thing to be passed
through.

Below are a few things you can do with the pi variable. In these protos,
wr should be one of the communicator inside pi.
  pi_hello(pi)           // a small hello-world that tests the pi
  pi_thread_bcast(void* ptr, size_t size, BWC_PI_BYTE, uint root, wr)
                        // Broadcasting function at thread level
  pi_bcast(...)
                        // Broadcasting function at MPI+thread level
  serialize(wr)         // Serializing stuff
  serialize_threads(wr)
The most interesting example of using these is matmul_top.[ch] . Be
careful: in these files, the pi's communicators are completed by others
communicators that links pieces of the matrix. In particular, there are
two intersecting communicators which are sub-communicators of the larger
one. This implies that serializing on one of these sub-communicators with
all threads of the other communicator concurrently is illegal, because
serialize() entails MPI calls which do not handle concurrency well.

The "abase" mechanism
---------------------

The matrix stuff is kind-of generic w.r.t the type of objects inside the
matrix. The "abase" mechanism provides this genericity -- don't expect a
clean ring structure, beautifully following the mathematical notions.
This layer is built from the mpfq library, together with a table of
indirect functions serving as wrappers.

Thanks to the abase layer, the same code may be used both in
characteristic two and in characteristic p, only with minor changes.

The abase layer is accessed in different ways depending on whether we're
looking at a higher level or lower level part of the code.

For the higher level code (typically, matmul_top.c, krylov.c, etc), we
have some object-oriented access. Everything goes through the
mpfq_vbase_ptr type, which is the type of the parent structure, holding
all the required function pointers.  The elements are cast to void*
pointers, will all the accompanying infelicities. The cod compiled in
this way is independent of the actual implementation of the underlying
arithmetic.

For lower level code (typically, matmul-bucket.cpp), it's different. We
have the two main types:
    abdst_field - a descriptor of the field we're working with
    abelt       - an element.

Each instance of the abase mechanism is a set of files, as follows:
    mpfq/mpfq_<something>.c
    mpfq/mpfq_<something>.h
    mpfq/mpfq_<something>_t.c
    mpfq/mpfq_<something>_t.h
This may describe elements whose sizes are known at compile-time
(uint64_t), or blocks of them, of width possibly known only at
running-time (this is the case for the mpfq_pz layer).

The idea is to have genericity and variable width for boring stuff (like
prep) that does not need maximal efficiency, and fixed sized- code for
the main operations inside Krylov.

See the example uses of the abase layer in either high level or low level
code for details on how to use this. Functions are all following the mpfq
api.

Currently the following are implemented:
  abase-u64     // uint64_t
  abase-u128    // uint128_t, packed into SSE2 if possible.
  abase-u64k    // block of k unint64_t, size known at compile-time
  abase-u64n    // block of n unint64_t, size not known at compile-time

The main use of this type is for vectors to be multiplied by the binary
matrix. We have for instance in matmul.[ch] a function called:
  void matmul_mul(matmul_ptr M , void * B , void const * A , int d)
that computes B = M . A  (or transpose if d=1), where A and B are vectors
of "abase".

Matrix - vector product
-----------------------

The code that will run on a single thread is in matmul.[ch] and the
different implementations it encapsulates. For the moment:
    matmul-basic.[ch]
    matmul-sliced.{cpp,h}
    matmul-threaded.[ch]
    matmul-bucket.{cpp,h}       [ default ]

practically all cpu time is spent in these routines. Therefore, optimizing
these for a particular CPU/memory controller combination is very relevant. See
more on this particular step below in the ``low-level'' section below.

On top of this very basic mono-threaded matrix x vector product, the
matmul_top.[ch] mechanism handles the parallelizing: each node / thread
gets a piece of the matrix (produced by balance) and does its own small
block x partial_vector product, and then the partial results are
transmitted and collected.

The intersections.[ch] files are used to help the communications. In one
word, this describes how an horizontal vector (spread on various
nodes/threads) can be transposed in a vertical vector (also spread on
various nodes/threads). This is tricky and will be explained somewhere
else (the code is short, but beware of headaches).

Matrix - vector product -- low-level
------------------------------------

Among the different possible source file listed above, presumably only one
will be used for a single bwc run. All bwc binaries (well, the important ones:
prep secure krylov mksol gather) accept the mm_impl= argument which selects
the implementation to use.

The primary on-disk matrix is in trivial binary format (32-bit integer
indices for non-zero coefficients, each row prefixed by its length).
However, memory representation for fast computation is different. It
depends on the implementation. For convenience, a ``cached'' matrix file
matching the in-memory representation for a given implementation is
generated on first use, and saved on disk

[ Note that this single task can be benched separately of the bw
  infrastructure by the standalone bench program. Be aware though that
  some of the difficulty performance-wise arises when all cpu cores are
  trying to interact with memory simultaneously. ]

(documentation to be continued).

Matrix - vector product -- high-level (1)
-----------------------------------------

The matmul_top_data type is responsible of tying together the different
threads and nodes, so as to work collectively on a matrix times vector
product.

The matmul_top_data object (generally abbreviated mmt) contains in
particular
 - on each thread/job, a pointer to the local share of the matrix.
 - communicator info in all directions
 - Two vector areas, a "left vector" and a "row vector". Those are quite
   often, and admittedly disconcertingly, named "row vector" and "column
   vector". In fact:
    - the left vector has (both collectively and locally) as many entries
      as the matrix has rows, and ditto for the right vector and columns.
    - the left vector has the proper size to be used in a vector times
      matrix product. The output of such a product goes to the right
      vector.
    - conversely, the right vector is fine for being the input to a
      matrix times vector product, whose output would go to the left
      vector.

The left vector is mmt->wr[0]->v
The right vector is mmt->wr[1]->v

In matmul_top.[ch], the same code can do right or left multiplications. A
parameter, usally called "d" means:
  d=0: vector times matrix, i.e. a vector is a row. Used for left
       nullspace computations (in factorization, for instance)
  d=1: matrix times vector, i.e. a vector is a column. Used for right
       nullspace computations (in discrete log)
Most of the comments in the source code takes the "d=1" point-of-view,
even though "d=0" is probably the most extensively tested.
This convention is visible also in the main bw_param structure. In the
matmul_top.[ch], it also indicates where the source vector is. Such a
point of view is also visible in the naming of variables, e.g. when we
write:
    mmt_comm_ptr mcol = mmt->wr[d];
this does actually correspond to a column only when d==1.


The main function is matmul_top_mul(), which decomposes in two steps:
  matmul_top_mul_cpu(), which does no communication
  matmul_top_mul_comm(), which does (almost) no computation

In a Wiedemann setting, bw->dir (nullspace=... at the top-level) should
be the same as the d-parameter passed to matmul_top_mul().

In a Lanczos setting, matmul_top_mul() is called in both directions.

Apart from this d=[0|1] nightmare, the mul_cpu() just calls matmul_mul(),
and put the result in the destination (partial) vector.  The tricky part
is (of course) the mul_comm() part. A brief on vector layout first.

Vector layout
-------------

The matmul_top layer distributes vector data across jobs and threads, in
such a way that prior to doing the local matrix product, each vector has
all the required input data.

Suppose an N*N matrix is split into a mesh of nh rows and nv columns, and
suppose we're talking about a vector times matrix product.

Each threads wants its share of size N/nh of the left vector. Note though
that there are, in total, nv threads which want the same data set for
input. Among these, we define each thread to be the "owner" of a fraction
of size N/(nh*nv).


We define several states of consistency for a vector:
 - minimal: each thread has the data it owns correct.
 - maximal: each thread has all input data it might need correct.


Matrix - vector product -- high-level (1)
-----------------------------------------

The cpu-intensive step requires maximal consistency on the input side.
Once the local multiplication is completed, we have no consistency on the
output side, because the data must be reconciled.

Two phases of communication are defined in order to reach maximal
consistency again.
  - reduce_across()
  - broadcast_down()
It takes as input the result of the cpu step, which is in the
"destination" vector, and does all the communications to reconstruct the
vector for the next iteration (thus, stored again in the "source"
vector).

reduce_across is actually a slight misnomer. The real operation is a
"reduce-scatter" operation, in MPI parlance. It adds all data, and sends
each share of the resulting vector to its owner. Of course, it's really
"across" only in case we're doing matrix times vector.

likewise, broadcast_down is more accurately an MPI "all-gather"
operation. Each thread sends the data it owns to all threads which are
going to need it.

Note that the vector layout which has been presented has the effect that
if the matrix is in fact the identity, then matmul_top_mul() will not
compute exactly the expected w = v * Id, but a shuffle of it, which
involves some transposition of indices. In other words, what is computed
correspond to a matrix M' = P*M, where P is a permutation matrix. This
reduces / simplifies / accelerates the mul_comm() part, and if the goal
is to find a kernel vector, knowing one for M' is equivalent to knowing
one for M.



*** Reduce_accross:

If at the top-level, one has d=1 (nullspace=right), then reduce_accross()
will be called with d=0, i.e. we want to reduce accross rows. 
First, the reduction is done at the thread level. This is just some
additions of vectors, done in the appropriate places of memory. Advantage
is taken of multithread, here: each thread is in charge of a subset of
the rows. This done in-place (i.e., still in the dst-vector).
Second, the reduction is done at the mpi level. Here, only the thread 0
is working in each node. This is done with MPI_Reduce_scatter (or any
replacement routine). And this is where the shuffle is implicitly
started. Indeed, each job will take care of reducing only a portion of
its indices. And therefore, in the end, the pieces of the vector are not
where they should be.
The reduction at the MPI level is where the result is written back in the
src vector in order to be ready for the next iteration.

*** Broadcast_down:

The broadcast is simply a nop at the thread level, since with shared
memory, everyone sees everyone else's data.
At the MPI level, though, again, there is some work to do, and only the
thread number 0 in each node is in charge. The MPI primitive that is used
here is MPI_Allgather. If at the top-level on has d=1, then this is
called with d=1, i.e. the communications are down within columns.

