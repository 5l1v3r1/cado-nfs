The pi_go() function
--------------------

This is the main workhorse for the parallelizing of the computation. It
allows multi-thread (with pthread) and multinode (with MPI) processing.

The parameters given to pi_go() are:
   . a pointer to the function that must be called in parallel.
   . the mpi splitting (a pair of integers)
   . the thread splitting (a pair of integers)

Its implementation is in parallelizing_info.[ch] .  The main data
structure that conveys information on the parallel setting is 
  struct parallelizing_info_s
The details are not so simple... but in a nutshell, we get 3 "wirings",
the meaning of which is more or less a generalization of "MPI_Comm" to
encompass also the pthread level. The first wiring allows full
communication, the second is for row-wise communications, the third is
for column-wise communications.

The prototype of the function passed to pi_go() is
  void *(*fcn)(parallelizing_info_ptr pi, void * arg)
where parallelizing_info_ptr pi is built by pi_go and passed to the
function. The last argument allows any additional thing to be passed
through.

Below are a few things you can do with the pi variable. In these protos,
wr should be one of the wiring inside pi.
  hello(pi)             // a small hello-world that tests the pi
  thread_broadcast(wr, void** ptr, uint i)
                        // Broadcasting function at thread level
  global_broadcast(wr, ...)
                        // Broadcasting function at MPI+thread level
  serialize(wr)         // Serializing stuff
  serialize_threads(wr)
The most interesting example of using these is matmul_top.[ch] . Be
carfeful: in these files, the pi's wirings are completed by others
wirings that links pieces of the matrix.

The "abase" mechanism
---------------------

## THIS IS PARTLY OBSOLETE. THE GENERAL IDEA STILL HOLDS, BUT THE
## TOPLEVEL CODE NOW EMPLOYS AN ADDITIONAL INDIRECTION LAYER.

The matrix stuff is kind-of generic w.r.t the type of objects inside the
matrix. The "abase" mechanism provides this genericity -- don't expect a
clean ring structure, beautifully following the mathematical notions.

The main idea is to use a block of bits as a main type that corresponds
to the machine word-size, typically uint64_t on a 64 bit computer.

Two main types:
  abobj               // the parent structure (may be NULL)
  abt                   // an element

Each instance of the abase mechanism is a pair abase-xxx.[ch] . This may
describe elements whose sizes are known at compile-time (uint64_t), or
block of them, of width possibly known only at running-time.

The idea is to have genericity and variable width for boring stuff (like
prep) that does not need maximal efficiency, and fixed sized- code for
the main operations inside Krylov.

See the header of abase-api.h for details on how to use this.

Currently the following are implemented:
  abase-u64     // uint64_t
  abase-u128    // uint128_t, packed into SSE2 if possible.
  abase-u64k    // block of k unint64_t, size known at compile-time
  abase-u64n    // block of n unint64_t, size not known at compile-time

The main use of this type is for vectors to be multiplied by the binary
matrix. We have for instance in matmul.[ch] a fonction called:
  void matmul_mul(matmul_ptr M , abt * B , abt const * A , int d)
that computes B = M . A  (or transpose of d=1), where A and B are vectors
of "abase".

Matrix - vector product
-----------------------

The code that will run on a single thread is in matmul.[ch] and the
different implementations it encapsulates. For the moment:
    matmul-basic.[ch]
    matmul-sliced.{cpp,h}
    matmul-threaded.[ch]
    matmul-bucket.{cpp,h}       [ default ]

practically all cpu time is spent in these routines. Therefore, optimizing
these for a particular CPU/memory controller combination is very relevant. See
more on this particular step below in the ``low-level'' section below.

On top of this very basic mono-threaded matrix x vector product, the
matmul_top.[ch] mechanism handles the parallelizing: each node / thread
gets a piece of the matrix (produced by balance) and does its own small
block x partial_vector product, and then the partial results are
transmitted and collected.

The intersections.[ch] files are used to help the communications. In one
word, this describes how an horizontal vector (spread on various
nodes/threads) can be transposed in a vertical vector (also spread on
various nodes/threads). This is tricky and will be explained somewhere
else (the code is short, but beware of headaches).

Matrix - vector product -- low-level
------------------------------------

Among the different possible source file listed above, presumably only one
will be used for a single bwc run. All bwc binaries (well, the important ones:
prep secure krylov mksol gather) accept the mm_impl= argument which selects
the implementation to use.

The primary on-disk matrix is in trivial, easy-to-read, ascii format. However,
memory representation for fast computation is different. It depends on the
implementation. For convenience, a ``cached'' matrix file matching the
in-memory representation for a given implementation is generated on first
use, and saved on disk

[ Note that this single task can be benched separately of the bw
  infrastructure by the standalone bench program. Be aware though that
  some of the difficuly performance-wise arises when all cpu cores are
  trying to interact with memory simultaneously. ]

(documentation to be continued).
