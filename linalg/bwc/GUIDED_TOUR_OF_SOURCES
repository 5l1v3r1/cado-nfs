The pi_go() function
--------------------

This is the main workhorse for the parallelizing of the computation. It
allows multi-thread (with pthread) and multinode (with MPI) processing.

The parameters given to pi_go() are:
   . a pointer to the function that must be called in parallel.
   . the mpi splitting (a pair of integers)
   . the thread splitting (a pair of integers)

Its implementation is in parallelizing_info.[ch] .  The main data
structure that conveys information on the parallel setting is 
  struct parallelizing_info_s
The details are not so simple... but in a nutshell, we get 3 "wirings",
the meaning of which is more or less a generalization of "MPI_Comm" to
encompass also the pthread level. The first wiring allows full
communication, the second is for row-wise communications, the third is
for column-wise communications.

The prototype of the function passed to pi_go() is
  void *(*fcn)(parallelizing_info_ptr pi, void * arg)
where parallelizing_info_ptr pi is built by pi_go and passed to the
function. The last argument allows any additional thing to be passed
through.

Below are a few things you can do with the pi variable. In these protos,
wr should be one of the wiring inside pi.
  hello(pi)             // a small hello-world that tests the pi
  thread_broadcast(wr, void** ptr, uint i)
                        // Broadcasting function at thread level
  global_broadcast(wr, ...)
                        // Broadcasting function at MPI+thread level
  serialize(wr)         // Serializing stuff
  serialize_threads(wr)
The most interesting example of using these is matmul_top.[ch] . Be
careful: in these files, the pi's wirings are completed by others
wirings that links pieces of the matrix.

The "abase" mechanism
---------------------

## THIS IS PARTLY OBSOLETE. THE GENERAL IDEA STILL HOLDS, BUT THE
## TOPLEVEL CODE NOW EMPLOYS AN ADDITIONAL INDIRECTION LAYER.

The matrix stuff is kind-of generic w.r.t the type of objects inside the
matrix. The "abase" mechanism provides this genericity -- don't expect a
clean ring structure, beautifully following the mathematical notions.

The main idea is to use a block of bits as a main type that corresponds
to the machine word-size, typically uint64_t on a 64 bit computer.

Two main types:
  abobj               // the parent structure (may be NULL)
  abt                   // an element

Each instance of the abase mechanism is a pair abase-xxx.[ch] . This may
describe elements whose sizes are known at compile-time (uint64_t), or
block of them, of width possibly known only at running-time.

The idea is to have genericity and variable width for boring stuff (like
prep) that does not need maximal efficiency, and fixed sized- code for
the main operations inside Krylov.

See the header of abase-api.h for details on how to use this.

Currently the following are implemented:
  abase-u64     // uint64_t
  abase-u128    // uint128_t, packed into SSE2 if possible.
  abase-u64k    // block of k unint64_t, size known at compile-time
  abase-u64n    // block of n unint64_t, size not known at compile-time

The main use of this type is for vectors to be multiplied by the binary
matrix. We have for instance in matmul.[ch] a function called:
  void matmul_mul(matmul_ptr M , abt * B , abt const * A , int d)
that computes B = M . A  (or transpose if d=1), where A and B are vectors
of "abase".

Matrix - vector product
-----------------------

The code that will run on a single thread is in matmul.[ch] and the
different implementations it encapsulates. For the moment:
    matmul-basic.[ch]
    matmul-sliced.{cpp,h}
    matmul-threaded.[ch]
    matmul-bucket.{cpp,h}       [ default ]

practically all cpu time is spent in these routines. Therefore, optimizing
these for a particular CPU/memory controller combination is very relevant. See
more on this particular step below in the ``low-level'' section below.

On top of this very basic mono-threaded matrix x vector product, the
matmul_top.[ch] mechanism handles the parallelizing: each node / thread
gets a piece of the matrix (produced by balance) and does its own small
block x partial_vector product, and then the partial results are
transmitted and collected.

The intersections.[ch] files are used to help the communications. In one
word, this describes how an horizontal vector (spread on various
nodes/threads) can be transposed in a vertical vector (also spread on
various nodes/threads). This is tricky and will be explained somewhere
else (the code is short, but beware of headaches).

Matrix - vector product -- low-level
------------------------------------

Among the different possible source file listed above, presumably only one
will be used for a single bwc run. All bwc binaries (well, the important ones:
prep secure krylov mksol gather) accept the mm_impl= argument which selects
the implementation to use.

The primary on-disk matrix is in trivial binary format (32-bit integer
indices for non-zero coefficients, each row prefixed by its length).
However, memory representation for fast computation is different. It
depends on the implementation. For convenience, a ``cached'' matrix file
matching the in-memory representation for a given implementation is
generated on first use, and saved on disk

[ Note that this single task can be benched separately of the bw
  infrastructure by the standalone bench program. Be aware though that
  some of the difficulty performance-wise arises when all cpu cores are
  trying to interact with memory simultaneously. ]

(documentation to be continued).

Matrix - vector product -- high-level
-------------------------------------

The files matmul_top.[ch] do the high-level stuff.
Things are complicated by the fact that the same code can do right or
left multiplications. A parameter, usally called "d" means:
  d=0: vector times matrix, i.e. a vector is a row. Used for left
       nullspace computations (in factorization, for instance)
  d=1: matrix times vector, i.e. a vector is a column. Used for right
       nullspace computations (in discrete log)
Most of the comments in the source code takes the "d=1" point-of-view,
even though "d=0" is probably the most extensively tested.
This convention is visible also in the main bw_param structure. In the
matmul_top.[ch], it also indicates where the source vector is.

The main function is matmul_top_mul(), which decomposes in two steps:
  matmul_top_mul_cpu(), which does no communication
  matmul_top_mul_comm(), which does (almost) no computation

Each thread has room for two pieces of vectors (src and dst). The vector
number 1 is the source when doing nullspace=right (d=1), and the
destination otherwise. [ in a sense, vector 0 is seen as a row,
and vector 1 as a column; although this does not really make sense, we
can see such a convention in the source as well ]

In a Wiedemann setting, bw->dir (nullspace=... at the top-level) should
be the same as the d-parameter passed to matmul_top_mul(). However, if a
Lanczos implementation end-up being implemented, this will refer to
separate notions.

Apart from this d=[0|1] nightmare, the mul_cpu() just calls matmul_mul(),
and put the result in the destination (partial) vector.  The tricky part
is (of course) the mul_comm() part.

The communication phase is split in two phases:
  - reduce_accross()
  - broadcast_down()
It takes as input the result of the cpu step, which is in the
"destination" vector, and does all the communications to reconstruct the
vector for the next iteration (thus, stored again in the "source"
vector).

Here, another complication occurs: there is a flag called
FLAG_SHUFFLED_MUL (that corresponds to the --shuffled-product option of
mf_bal.c and bwc.pl). If it is set, then matmul_top_mul() will not
compute exactly the expected w = M*v product but a shuffle of it. In
other words, what is computed correspond to a matrix M' = P*M, where P is
a permutation matrix. This reduces / simplifies the mul_comm() part, and
if the goal is to find a kernel vector, knowing one for M' is
equivalent to knowing one for M.
In the following, we assume that we are indeed in the case where
FLAG_SHUFFLED_MUL is activated.

*** Reduce_accross:

If at the top-level, one has d=1 (nullspace=right), then reduce_accross()
will be called with d=0, i.e. we want to reduce accross rows. 
First, the reduction is done at the thread level. This is just some
additions of vectors, done in the appropriate places of memory. Advantage
is taken of multithread, here: each thread is in charge of a subset of
the rows. This done in-place (i.e., still in the dst-vector).
Second, the reduction is done at the mpi level. Here, only the thread 0
is working in each node. This is done with MPI_Reduce_scatter (or any
replacement routine). And this is where the shuffle is implicitly
started. Indeed, each job will take care of reducing only a portion of
its indices. And therefore, in the end, the pieces of the vector are not
where they should be.
The reduction at the MPI level is where the result is written back in the
src vector in order to be ready for the next iteration.

*** Broadcast_down:

The broadcast is simply a nop at the thread level, since with shared
memory, everyone sees everyone else's data.
At the MPI level, though, again, there is some work to do, and only the
thread number 0 in each node is in charge. The MPI primitive that is used
here is MPI_Allgather. If at the top-level on has d=1, then this is
called with d=1, i.e. the communications are down within columns.

