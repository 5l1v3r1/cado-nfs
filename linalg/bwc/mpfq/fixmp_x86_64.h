/*
 * Library MPFq, package mp.
 * Multiprecision routines (small number of words) for x86_64. 
 * Automatically generated by perl/gen_mp_x86_64.pl 
 */

#ifndef __MP_X86_64_H__
#define __MP_X86_64_H__

#define HAVE_NATIVE_ADDMUL1_NC_1 1
static void 
addmul1_nc_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %1\n"
  : "+rm" (z[0]), "+rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}


#define HAVE_NATIVE_ADDMUL1_NC_2 1
static void
addmul1_nc_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %3, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %4, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %2\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2])
  : "m" (x[0]), "m" (x[1]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_3 1
static void
addmul1_nc_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %4, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %5, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %6, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %3\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_4 1
static void
addmul1_nc_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %5, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %6, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %7, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %8, %%rax\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %3\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, %4\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3]), "+m" (z[4])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_1HW 1
static void 
addmul1_nc_1hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, %1\n"
  : "+rm" (z[0]), "+rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_2HW 1
static void
addmul1_nc_2hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %3, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, %1\n"
  : "+m" (z[0]), "+m" (z[1])
  : "m" (x[0]), "m" (x[1]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_3HW 1
static void
addmul1_nc_3hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %3, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %4, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %5, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, %2\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_4HW 1
static void
addmul1_nc_4hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %4, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %5, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %6, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %7, %%rax\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, %3\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
}

#define HAVE_NATIVE_ADDMUL1_NC_5 1
static void
addmul1_nc_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_6 1
static void
addmul1_nc_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_7 1
static void
addmul1_nc_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_8 1
static void
addmul1_nc_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_9 1
static void
addmul1_nc_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    addq    %%rcx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_5HW 1
static void
addmul1_nc_5hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_6HW 1
static void
addmul1_nc_6hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_7HW 1
static void
addmul1_nc_7hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_8HW 1
static void
addmul1_nc_8hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_NC_9HW 1
static void
addmul1_nc_9hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADDMUL1_1 1
static mp_limb_t 
addmul1_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %3, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %[ret]\n"
  : "+rm" (z[0]), "+rm" (z[1]), [ret] "+rm" (ret)
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_2 1
static mp_limb_t
addmul1_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %4, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %5, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_3 1
static mp_limb_t
addmul1_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %5, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %6, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %7, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %3\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_4 1
static mp_limb_t
addmul1_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %6, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %7, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %8, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %9, %%rax\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, %3\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    addq    %%rcx, %4\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3]), "+m" (z[4]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_1HW 1
static mp_limb_t 
addmul1_1hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    adcq    $0, %[ret]\n"
  : "+rm" (z[0]), [ret] "+rm" (ret)
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_2HW 1
static mp_limb_t
addmul1_2hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %3, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %4, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_3HW 1
static mp_limb_t
addmul1_3hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %4, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %5, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %6, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_4HW 1
static mp_limb_t
addmul1_4hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %5, %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, %0\n"
   "    movq    %6, %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %7, %%rax\n"
   "    addq    %%rcx, %1\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %8, %%rax\n"
   "    addq    %%rcx, %2\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, %3\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z[0]), "+m" (z[1]), "+m" (z[2]), "+m" (z[3]), [ret] "+rm" (ret)
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_5 1
static mp_limb_t
addmul1_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, 40(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_6 1
static mp_limb_t
addmul1_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, 48(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_7 1
static mp_limb_t
addmul1_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, 56(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_8 1
static mp_limb_t
addmul1_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, 64(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_9 1
static mp_limb_t
addmul1_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rdx, 72(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_5HW 1
static mp_limb_t
addmul1_5hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_6HW 1
static mp_limb_t
addmul1_6hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_7HW 1
static mp_limb_t
addmul1_7hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_8HW 1
static mp_limb_t
addmul1_8hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_ADDMUL1_9HW 1
static mp_limb_t
addmul1_9hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
    mp_limb_t ret = 0;
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %2, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    addq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 8(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %[ret]\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %[ret]\n"
  : "+m" (z), [ret] "+r" (ret)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
  return ret;
}

#define HAVE_NATIVE_MUL1_1 1
static void 
mul1_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %2, %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, %0\n"
   "    movq    %%rdx, %1\n"
  : "=rm" (z[0]), "=rm" (z[1])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rdx");
}

#define HAVE_NATIVE_MUL1_2 1
static void
mul1_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, 16(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_3 1
static void
mul1_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_4 1
static void
mul1_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_5 1
static void
mul1_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_6 1
static void
mul1_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_7 1
static void
mul1_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_8 1
static void
mul1_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_9 1
static void
mul1_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
   "    movq    %%rdx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_1HW 1
static void 
mul1_1hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %1, %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, %0\n"
  : "=rm" (z[0])
  : "rm" (x[0]), [mult] "r" (c)
  : "%rax", "%rdx");
}

#define HAVE_NATIVE_MUL1_2HW 1
static void
mul1_2hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_3HW 1
static void
mul1_3hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_4HW 1
static void
mul1_4hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_5HW 1
static void
mul1_5hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_6HW 1
static void
mul1_6hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_7HW 1
static void
mul1_7hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_8HW 1
static void
mul1_8hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL1_9HW 1
static void
mul1_9hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t c)
{
  __asm__ volatile(
   "    movq    %0, %%rdi\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %[mult]\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"

   "    mulq    %[mult]\n"
   "    addq    %%rax, %%rcx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), [mult] "r" (c)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_NC_1 1
static void
add_nc_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  *z = *x + *y;
}

#define HAVE_NATIVE_ADD_NC_2 1
static void
add_nc_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %2, %%rax\n"
    "   addq    %4, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %3, %%rax\n"
    "   adcq    %5, %%rax\n"
    "   movq    %%rax, %1\n"
  : "=m" (z[0]), "=m" (z[1])
  : "m" (x[0]), "m" (x[1]), "m" (y[0]), "m" (y[1])
  : "%rax");
}

#define HAVE_NATIVE_ADD_NC_3 1
static void
add_nc_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %3, %%rax\n"
    "   addq    %6, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %4, %%rax\n"
    "   adcq    %7, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %5, %%rax\n"
    "   adcq    %8, %%rax\n"
    "   movq    %%rax, %2\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (y[0]), "m" (y[1]), "m" (y[2])
  : "%rax");
}

#define HAVE_NATIVE_ADD_NC_4 1
static void
add_nc_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %4, %%rax\n"
    "   addq    %8, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %5, %%rax\n"
    "   adcq    %9, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %6, %%rax\n"
    "   adcq    %10, %%rax\n"
    "   movq    %%rax, %2\n"

    "   movq    %7, %%rax\n"
    "   adcq    %11, %%rax\n"
    "   movq    %%rax, %3\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2]), "=m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), "m" (y[0]), "m" (y[1]), "m" (y[2]), "m" (y[3])
  : "%rax");
}

#define HAVE_NATIVE_ADD_NC_5 1
static void
add_nc_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_NC_6 1
static void
add_nc_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_NC_7 1
static void
add_nc_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_NC_8 1
static void
add_nc_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   adcq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_ADD_NC_9 1
static void
add_nc_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   addq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   adcq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   adcq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   adcq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   adcq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   adcq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   adcq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   adcq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"

    "   movq    64(%%rsi), %%rax\n"
    "   adcq    64(%%rdx), %%rax\n"
    "   movq    %%rax, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_NC_1 1
static void
sub_nc_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  *z = *x - *y;
}

#define HAVE_NATIVE_SUB_NC_2 1
static void
sub_nc_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %2, %%rax\n"
    "   subq    %4, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %3, %%rax\n"
    "   sbbq    %5, %%rax\n"
    "   movq    %%rax, %1\n"
  : "=m" (z[0]), "=m" (z[1])
  : "m" (x[0]), "m" (x[1]), "m" (y[0]), "m" (y[1])
  : "%rax");
}

#define HAVE_NATIVE_SUB_NC_3 1
static void
sub_nc_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %3, %%rax\n"
    "   subq    %6, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %4, %%rax\n"
    "   sbbq    %7, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %5, %%rax\n"
    "   sbbq    %8, %%rax\n"
    "   movq    %%rax, %2\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (y[0]), "m" (y[1]), "m" (y[2])
  : "%rax");
}

#define HAVE_NATIVE_SUB_NC_4 1
static void
sub_nc_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %4, %%rax\n"
    "   subq    %8, %%rax\n"
    "   movq    %%rax, %0\n"

    "   movq    %5, %%rax\n"
    "   sbbq    %9, %%rax\n"
    "   movq    %%rax, %1\n"

    "   movq    %6, %%rax\n"
    "   sbbq    %10, %%rax\n"
    "   movq    %%rax, %2\n"

    "   movq    %7, %%rax\n"
    "   sbbq    %11, %%rax\n"
    "   movq    %%rax, %3\n"
  : "=m" (z[0]), "=m" (z[1]), "=m" (z[2]), "=m" (z[3])
  : "m" (x[0]), "m" (x[1]), "m" (x[2]), "m" (x[3]), "m" (y[0]), "m" (y[1]), "m" (y[2]), "m" (y[3])
  : "%rax");
}

#define HAVE_NATIVE_SUB_NC_5 1
static void
sub_nc_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_NC_6 1
static void
sub_nc_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_NC_7 1
static void
sub_nc_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_NC_8 1
static void
sub_nc_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   sbbq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_SUB_NC_9 1
static void
sub_nc_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile (
    "   movq    %0, %%rdi\n"
    "   movq    %1, %%rsi\n"
    "   movq    %2, %%rdx\n"
    "   movq    (%%rsi), %%rax\n"
    "   subq    (%%rdx), %%rax\n"
    "   movq    %%rax, (%%rdi)\n"

    "   movq    8(%%rsi), %%rax\n"
    "   sbbq    8(%%rdx), %%rax\n"
    "   movq    %%rax, 8(%%rdi)\n"

    "   movq    16(%%rsi), %%rax\n"
    "   sbbq    16(%%rdx), %%rax\n"
    "   movq    %%rax, 16(%%rdi)\n"

    "   movq    24(%%rsi), %%rax\n"
    "   sbbq    24(%%rdx), %%rax\n"
    "   movq    %%rax, 24(%%rdi)\n"

    "   movq    32(%%rsi), %%rax\n"
    "   sbbq    32(%%rdx), %%rax\n"
    "   movq    %%rax, 32(%%rdi)\n"

    "   movq    40(%%rsi), %%rax\n"
    "   sbbq    40(%%rdx), %%rax\n"
    "   movq    %%rax, 40(%%rdi)\n"

    "   movq    48(%%rsi), %%rax\n"
    "   sbbq    48(%%rdx), %%rax\n"
    "   movq    %%rax, 48(%%rdi)\n"

    "   movq    56(%%rsi), %%rax\n"
    "   sbbq    56(%%rdx), %%rax\n"
    "   movq    %%rax, 56(%%rdi)\n"

    "   movq    64(%%rsi), %%rax\n"
    "   sbbq    64(%%rdx), %%rax\n"
    "   movq    %%rax, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rdx", "%rsi", "%rdi", "memory");
}

#define HAVE_NATIVE_MUL_1 1
static void 
mul_1(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "    mulq %3\n"
  : "=a" (z[0]), "=d" (z[1])
  : "0" (x[0]), "rm1" (y[0])
  : "cc");
}

#define HAVE_NATIVE_MUL_2 1
static void
mul_2(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, 16(%%rdi)\n"
   "	movq	$0, 24(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_3 1
static void
mul_3(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, 24(%%rdi)\n"
   "	movq	$0, 32(%%rdi)\n"
   "	movq	$0, 40(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 40(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_4 1
static void
mul_4(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "	movq	$0, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_5 1
static void
mul_5(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_6 1
static void
mul_6(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_7 1
static void
mul_7(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_8 1
static void
mul_8(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "	movq	$0, 120(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 120(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_9 1
static void
mul_9(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "	movq	$0, 120(%%rdi)\n"
   "	movq	$0, 128(%%rdi)\n"
   "	movq	$0, 136(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 120(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 128(%%rdi)\n"
   "  ### x*y[8]\n"
   "    movq	64(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 64(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 128(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 136(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_1HW 1
static void 
mul_1hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  z[0]=x[0]*y[0];
}

#define HAVE_NATIVE_MUL_2HW 1
static void
mul_2hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, 16(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 16(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_3HW 1
static void
mul_3hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, 24(%%rdi)\n"
   "	movq	$0, 32(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_4HW 1
static void
mul_4hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, 32(%%rdi)\n"
   "	movq	$0, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_5HW 1
static void
mul_5hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	movq	$0, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_6HW 1
static void
mul_6hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, 48(%%rdi)\n"
   "	movq	$0, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_7HW 1
static void
mul_7hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "	movq	$0, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_8HW 1
static void
mul_8hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, 64(%%rdi)\n"
   "	movq	$0, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_MUL_9HW 1
static void
mul_9hw(mp_limb_t *z, const mp_limb_t *x, const mp_limb_t *y)
{
  __asm__ volatile(
   "  ### START OF MUL\n"
   "  ### x*y[0]\n"
   "    movq	%2, %%r8\n"
   "    movq    %0, %%rdi\n"
   "    movq	(%%r8), %%r9\n"
   "    movq    %1, %%rsi\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    movq    %%rax, (%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    movq    %%rcx, 8(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    movq    %%rcx, 16(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    movq    %%rcx, 24(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    movq    %%rcx, 32(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    movq    %%rcx, 40(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    movq    %%rcx, 48(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    movq    %%rcx, 56(%%rdi)\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rcx, 64(%%rdi)\n"
   "    movq    %%rdx, 72(%%rdi)\n"
   "	movq	$0, 80(%%rdi)\n"
   "	movq	$0, 88(%%rdi)\n"
   "	movq	$0, 96(%%rdi)\n"
   "	movq	$0, 104(%%rdi)\n"
   "	movq	$0, 112(%%rdi)\n"
   "	movq	$0, 120(%%rdi)\n"
   "	movq	$0, 128(%%rdi)\n"
   "  ### x*y[1]\n"
   "    movq	8(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 8(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 16(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 80(%%rdi)\n"
   "  ### x*y[2]\n"
   "    movq	16(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 16(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 24(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 88(%%rdi)\n"
   "  ### x*y[3]\n"
   "    movq	24(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 24(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 32(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 96(%%rdi)\n"
   "  ### x*y[4]\n"
   "    movq	32(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 32(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 40(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 104(%%rdi)\n"
   "  ### x*y[5]\n"
   "    movq	40(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 40(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 48(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 112(%%rdi)\n"
   "  ### x*y[6]\n"
   "    movq	48(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 48(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 56(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 120(%%rdi)\n"
   "  ### x*y[7]\n"
   "    movq	56(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 56(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 64(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, 128(%%rdi)\n"
   "  ### x*y[8]\n"
   "    movq	64(%%r8), %%r9\n"
   "    movq    (%%rsi), %%rax\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, 64(%%rdi)\n"
   "    movq    8(%%rsi), %%rax\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    16(%%rsi), %%rax\n"
   "    addq    %%rcx, 72(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    24(%%rsi), %%rax\n"
   "    addq    %%rcx, 80(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    32(%%rsi), %%rax\n"
   "    addq    %%rcx, 88(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    40(%%rsi), %%rax\n"
   "    addq    %%rcx, 96(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    48(%%rsi), %%rax\n"
   "    addq    %%rcx, 104(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    56(%%rsi), %%rax\n"
   "    addq    %%rcx, 112(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    adcq    $0, %%rdx\n"
   "    movq    64(%%rsi), %%rax\n"
   "    addq    %%rcx, 120(%%rdi)\n"
   "    adcq    $0, %%rdx\n"
   "    movq    %%rdx, %%rcx\n"
   "    mulq    %%r9\n"
   "    addq    %%rax, %%rcx\n"
   "    addq    %%rcx, 128(%%rdi)\n"
  : "+m" (z)
  : "m" (x), "m" (y)
  : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_SQR_1 1
static void 
sqr_1(mp_limb_t *z, const mp_limb_t *x)
{
  mpfq_umul_ppmm(z[1], z[0], x[0], x[0]);
}

#define HAVE_NATIVE_SQR_2 1
static void 
sqr_2(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq    %1, %%r8\n"
   "	movq	%0, %%rdi\n"
   "	movq    (%%r8), %%rax\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%r8), %%rax\n"
   "	movq    %%rdx, %%r9\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, %%r10\n"
   "	movq    (%%r8), %%rax\n"
   "	movq    %%rdx, %%r11\n"
   "	mulq    8(%%r8)\n"
   "	addq    %%rax, %%r9\n"
   "	adcq    %%rdx, %%r10\n"
   "	adcq    $0, %%r11\n"
   "	addq    %%rax, %%r9\n"
   "	movq	%%r9, 8(%%rdi)\n"
   "	adcq    %%rdx, %%r10\n"
   "	movq	%%r10, 16(%%rdi)\n"
   "	adcq    $0, %%r11\n"
   "	movq	%%r11, 24(%%rdi)\n"
  : "+m" (z)
  : "m" (x) 
  : "%rax", "%rdx", "%rdi", "%r8", "%r9", "%r10", "%r11", "memory");
} 

#define HAVE_NATIVE_SQR_3 1
static void 
sqr_3(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1,2]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	### x[1]*x[2]\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	16(%%rsi)\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%rdx, %%rdx\n"
   "	adcq	$0, 40(%%rdi)\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%rdx, 32(%%rdi)\n"
   "	adcq	$0, 40(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_SQR_4 1
static void 
sqr_4(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "	movq	24(%%rsi), %%rax\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 48(%%rdi)\n"
   "    movq    %%rdx, 56(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1:3]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	movq    24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r11\n"
   "	### x[1]*x[2:3]\n"
   "	movq	8(%%rsi), %%rcx\n"
   "	movq	16(%%rsi), %%rax\n"
   "	xorq	%%r12, %%r12\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	%%rdx, %%r11\n"
   "	adcq	$0, %%r12\n"
   "	movq	24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq    %%rax, %%r11\n"
   "	adcq	$0, %%rdx\n"
   "	addq    %%rdx, %%r12\n"
   "	### x[2]*x[3]\n"
   "	movq	16(%%rsi), %%rax\n"
   "	mulq	24(%%rsi)\n"
   "	addq	%%rax, %%r12\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%r11, %%r11\n"
   "	adcq	%%r12, %%r12\n"
   "	adcq	%%rdx, %%rdx\n"
   "	adcq	$0, 56(%%rdi)\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%r11, 32(%%rdi)\n"
   "	adcq	%%r12, 40(%%rdi)\n"
   "	adcq	%%rdx, 48(%%rdi)\n"
   "	adcq	$0, 56(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10",
   "%r11", "%r12", "memory");
}

#define HAVE_NATIVE_SQR_1HW 1
static void 
sqr_1hw(mp_limb_t *z, const mp_limb_t *x)
{
  z[0]= x[0]* x[0];
}

#define HAVE_NATIVE_SQR_2HW 1
static void 
sqr_2hw(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq    %1, %%r8\n"
   "	movq	%0, %%rdi\n"
   "	movq    (%%r8), %%rax\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%r8), %%rax\n"
   "	movq    %%rdx, %%r9\n"
   "	mulq    %%rax\n"
   "	movq    %%rax, %%r10\n"
   "	movq    (%%r8), %%rax\n"
   "	mulq    8(%%r8)\n"
   "	addq    %%rax, %%r9\n"
   "	adcq    %%rdx, %%r10\n"
   "	addq    %%rax, %%r9\n"
   "    movq    %%r9, 8(%%rdi)\n"
   "	adcq    %%rdx, %%r10\n"
   "	movq	%%r10, 16(%%rdi)\n"
  : "+m" (z)
  : "m" (x) 
  : "%rax", "%rdx", "%rdi", "%r8", "%r9", "%r10", "memory");
} 

#define HAVE_NATIVE_SQR_3HW 1
static void 
sqr_3hw(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1,2]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	### x[1]*x[2]\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	16(%%rsi)\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%rdx, %%rdx\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%rdx, 32(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10", "memory");
}

#define HAVE_NATIVE_SQR_4HW 1
static void 
sqr_4hw(mp_limb_t *z, const mp_limb_t *x)
{
  __asm__ volatile(
   "	movq	%1, %%rsi\n"
   "	movq	%0, %%rdi\n"
   "	### diagonal elements\n"
   "	movq    (%%rsi), %%rax\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, (%%rdi)\n"
   "	movq    8(%%rsi), %%rax\n"
   "	movq    %%rdx, 8(%%rdi)\n"
   "	mulq	%%rax\n"
   "	movq    %%rax, 16(%%rdi)\n"
   "	movq	16(%%rsi), %%rax\n"
   "	movq    %%rdx, 24(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 32(%%rdi)\n"
   "	movq	24(%%rsi), %%rax\n"
   "    movq    %%rdx, 40(%%rdi)\n"
   "	mulq    %%rax\n"
   "    movq    %%rax, 48(%%rdi)\n"
   "	### precompute triangle\n"
   "	### x[0]*x[1:3]\n"
   "	movq	(%%rsi), %%rcx\n"
   "	movq	8(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	movq	%%rax, %%r8\n"
   "	movq	%%rdx, %%r9\n"
   "	movq    16(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r9\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r10\n"
   "	movq    24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	$0, %%rdx\n"
   "	movq	%%rdx, %%r11\n"
   "	### x[1]*x[2:3]\n"
   "	movq	8(%%rsi), %%rcx\n"
   "	movq	16(%%rsi), %%rax\n"
   "	xorq	%%r12, %%r12\n"
   "	mulq	%%rcx\n"
   "	addq	%%rax, %%r10\n"
   "	adcq	%%rdx, %%r11\n"
   "	adcq	$0, %%r12\n"
   "	movq	24(%%rsi), %%rax\n"
   "	mulq	%%rcx\n"
   "	addq    %%rax, %%r11\n"
   "	adcq	$0, %%rdx\n"
   "	addq    %%rdx, %%r12\n"
   "	### x[2]*x[3]\n"
   "	movq	16(%%rsi), %%rax\n"
   "	mulq	24(%%rsi)\n"
   "	addq	%%rax, %%r12\n"
   "	adcq	$0, %%rdx\n"
   "	### Shift triangle\n"
   "	addq	%%r8, %%r8\n"
   "	adcq	%%r9, %%r9\n"
   "	adcq	%%r10, %%r10\n"
   "	adcq	%%r11, %%r11\n"
   "	adcq	%%r12, %%r12\n"
   "	adcq	%%rdx, %%rdx\n"
   "	### add shifted triangle to diagonal\n"
   "	addq	%%r8, 8(%%rdi)\n"
   "	adcq	%%r9, 16(%%rdi)\n"
   "	adcq	%%r10, 24(%%rdi)\n"
   "	adcq	%%r11, 32(%%rdi)\n"
   "	adcq	%%r12, 40(%%rdi)\n"
   "	adcq	%%rdx, 48(%%rdi)\n"
   : "+m" (z)
   : "m" (x) 
   : "%rax", "%rcx", "%rdx", "%rsi", "%rdi", "%r8", "%r9", "%r10",
   "%r11", "%r12", "memory");
}

#define HAVE_NATIVE_MULREDC_1 1
static void
mulredc_1(mp_limb_t *pz, const mp_limb_t *px, const mp_limb_t *py,
    const mp_limb_t *pp, const mp_limb_t *pinvp) MAYBE_UNUSED;
static void
mulredc_1(mp_limb_t *pz, const mp_limb_t *px, const mp_limb_t *py,
    const mp_limb_t *pp, const mp_limb_t *pinvp)
{
    mp_limb_t x = *px;
    mp_limb_t y = *py;
    mp_limb_t p = *pp;
    mp_limb_t invp = *pinvp;
    mp_limb_t z;
    __asm__ volatile (
    "   movq    %[x], %%rax\n"
    "   mulq    %[y]\n"
    "   movq    %%rdx, %[z]\n"
    "   imul    %[invp], %%rax\n"
    "   mulq    %[p]\n"
    "   addq    $0xFFFFFFFFFFFFFFFF, %%rax\n" // set carry if ax is not 0
    "   adcq    $0, %[z]\n" // this should not produce any carry
    "   movq    %[z], %%rax\n" // ax = z
    "   subq    %[p], %%rax\n" // ax -= p
    "   addq    %%rdx, %[z]\n" // z += dx
    "   addq    %%rdx, %%rax\n" // ax += dx  (ax = z-p+dx)
    "   cmovc   %%rax, %[z]\n"  // z c_= ax
    : [z] "=&r" (z) // , [fix] "=r" (fix)
    : [x] "rm" (x), [y] "rm" (y), [p] "rm" (p), [invp] "rm" (invp)
    : "%rax", "%rdx"
    );
    *pz = z;
}

#endif  /* __MP_X86_64_H__ */
